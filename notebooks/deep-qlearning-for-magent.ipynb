{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:47:40.617234Z",
     "iopub.status.busy": "2024-12-08T00:47:40.616669Z",
     "iopub.status.idle": "2024-12-08T00:48:15.177048Z",
     "shell.execute_reply": "2024-12-08T00:48:15.175842Z",
     "shell.execute_reply.started": "2024-12-08T00:47:40.617197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:44.381725Z",
     "iopub.status.busy": "2024-12-08T00:48:44.380859Z",
     "iopub.status.idle": "2024-12-08T00:48:44.387582Z",
     "shell.execute_reply": "2024-12-08T00:48:44.386600Z",
     "shell.execute_reply.started": "2024-12-08T00:48:44.381676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from magent2.environments import battle_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:44.965935Z",
     "iopub.status.busy": "2024-12-08T00:48:44.965268Z",
     "iopub.status.idle": "2024-12-08T00:48:44.972211Z",
     "shell.execute_reply": "2024-12-08T00:48:44.971284Z",
     "shell.execute_reply.started": "2024-12-08T00:48:44.965903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, action_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:45.490423Z",
     "iopub.status.busy": "2024-12-08T00:48:45.490099Z",
     "iopub.status.idle": "2024-12-08T00:48:45.494793Z",
     "shell.execute_reply": "2024-12-08T00:48:45.493881Z",
     "shell.execute_reply.started": "2024-12-08T00:48:45.490397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:46.027502Z",
     "iopub.status.busy": "2024-12-08T00:48:46.027141Z",
     "iopub.status.idle": "2024-12-08T00:48:46.033533Z",
     "shell.execute_reply": "2024-12-08T00:48:46.032570Z",
     "shell.execute_reply.started": "2024-12-08T00:48:46.027470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PretrainedAgent:\n",
    "    def __init__(self, n_observation, n_actions, device=\"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.qnetwork = QNetwork(n_observation, n_actions).to(self.device)\n",
    "\n",
    "        self.qnetwork.load_state_dict(\n",
    "            torch.load(\"/kaggle/input/pretrained/pytorch/default/1/red.pt\", weights_only=True, map_location=self.device)\n",
    "        )\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        observation = (\n",
    "            torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            q_values = self.qnetwork(observation)\n",
    "        action = torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:46.631800Z",
     "iopub.status.busy": "2024-12-08T00:48:46.630950Z",
     "iopub.status.idle": "2024-12-08T00:48:46.643962Z",
     "shell.execute_reply": "2024-12-08T00:48:46.643077Z",
     "shell.execute_reply.started": "2024-12-08T00:48:46.631765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, observation_shape, action_shape, buffer_size=10000, batch_size=64, lr=1e-3, gamma=0.6, device=\"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.q_net = QNetwork(observation_shape, action_shape).float().to(self.device)\n",
    "        self.target_net = QNetwork(observation_shape, action_shape).float().to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_shape = action_shape\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_shape)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(observation).unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return self.q_net(state_tensor).argmax().item()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample the replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Filter out terminal states (actions == None)\n",
    "        valid_indices = [i for i in range(len(actions)) if actions[i] is not None]\n",
    "        if not valid_indices:  # If no valid samples, return\n",
    "            return\n",
    "\n",
    "        states = np.array([states[i] for i in valid_indices])\n",
    "        actions = np.array([actions[i] for i in valid_indices])\n",
    "        rewards = np.array([rewards[i] for i in valid_indices])\n",
    "        next_states = np.array([next_states[i] for i in valid_indices])\n",
    "        dones = np.array([dones[i] for i in valid_indices])\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.FloatTensor(states).permute(0, 3, 1, 2).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).permute(0, 3, 1, 2).to(self.device)\n",
    "        actions = torch.tensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss and update the network\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:47.611178Z",
     "iopub.status.busy": "2024-12-08T00:48:47.610789Z",
     "iopub.status.idle": "2024-12-08T00:48:47.617035Z",
     "shell.execute_reply": "2024-12-08T00:48:47.616081Z",
     "shell.execute_reply.started": "2024-12-08T00:48:47.611146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return (np.stack(state), np.array(action), np.array(reward), \n",
    "                np.stack(next_state), np.array(done))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T00:48:48.316767Z",
     "iopub.status.busy": "2024-12-08T00:48:48.316401Z",
     "iopub.status.idle": "2024-12-08T00:48:48.323591Z",
     "shell.execute_reply": "2024-12-08T00:48:48.322816Z",
     "shell.execute_reply.started": "2024-12-08T00:48:48.316736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(env, red_agent: DQNAgent, blue_agent: RandomAgent, episodes=500, target_update_freq=5):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, _ = env.last()\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "            else:\n",
    "                if agent == \"red_0\":\n",
    "                    action = red_agent.get_action(observation)\n",
    "                else:  # \"blue_0\"\n",
    "                    action = blue_agent.get_action(observation)\n",
    "\n",
    "            env.step(action)\n",
    "            if agent == \"red_0\":\n",
    "                red_agent.replay_buffer.push(\n",
    "                    observation, action, reward, env.last()[0], termination or truncation\n",
    "                )\n",
    "                total_reward += reward\n",
    "            red_agent.train()\n",
    "\n",
    "        red_agent.decay_epsilon()\n",
    "        if episode % target_update_freq == 0:\n",
    "            red_agent.update_target_network()\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {red_agent.epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-12-08T03:47:37.771746Z",
     "shell.execute_reply": "2024-12-08T03:47:37.770768Z",
     "shell.execute_reply.started": "2024-12-08T00:48:54.587436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 444, Total Reward: -2.33000008482486, Epsilon: 0.11\n",
      "Episode 445, Total Reward: -3.0350001147016883, Epsilon: 0.11\n",
      "Episode 446, Total Reward: -3.905000147409737, Epsilon: 0.11\n",
      "Episode 447, Total Reward: -3.700000138953328, Epsilon: 0.11\n",
      "Episode 448, Total Reward: -2.6500000907108188, Epsilon: 0.11\n",
      "Episode 449, Total Reward: -2.4250000827014446, Epsilon: 0.10\n",
      "Episode 450, Total Reward: -1.3650000412017107, Epsilon: 0.10\n",
      "Episode 451, Total Reward: -3.150000118650496, Epsilon: 0.10\n",
      "Episode 452, Total Reward: -2.9500001100823283, Epsilon: 0.10\n",
      "Episode 453, Total Reward: -2.5450000930577517, Epsilon: 0.10\n",
      "Episode 454, Total Reward: -2.5550000928342342, Epsilon: 0.10\n",
      "Episode 455, Total Reward: -2.790000100620091, Epsilon: 0.10\n",
      "Episode 456, Total Reward: -3.7100001387298107, Epsilon: 0.10\n",
      "Episode 457, Total Reward: -6.785000259056687, Epsilon: 0.10\n",
      "Episode 458, Total Reward: -1.7550000585615635, Epsilon: 0.10\n",
      "Episode 459, Total Reward: -3.1950001176446676, Epsilon: 0.10\n",
      "Episode 460, Total Reward: -1.970000066794455, Epsilon: 0.10\n",
      "Episode 461, Total Reward: -3.085000113584101, Epsilon: 0.10\n",
      "Episode 462, Total Reward: -2.4400000888854265, Epsilon: 0.10\n",
      "Episode 463, Total Reward: -3.2800001222640276, Epsilon: 0.10\n",
      "Episode 464, Total Reward: -3.3100001206621528, Epsilon: 0.10\n",
      "Episode 465, Total Reward: -2.020000072196126, Epsilon: 0.10\n",
      "Episode 466, Total Reward: -4.405000162310898, Epsilon: 0.10\n",
      "Episode 467, Total Reward: -2.570000092498958, Epsilon: 0.10\n",
      "Episode 468, Total Reward: -2.650000097230077, Epsilon: 0.10\n",
      "Episode 469, Total Reward: -1.9550000671297312, Epsilon: 0.10\n",
      "Episode 470, Total Reward: -3.370000126771629, Epsilon: 0.10\n",
      "Episode 471, Total Reward: -2.2850000793114305, Epsilon: 0.10\n",
      "Episode 472, Total Reward: -2.365000084042549, Epsilon: 0.10\n",
      "Episode 473, Total Reward: -2.2450000802055, Epsilon: 0.10\n",
      "Episode 474, Total Reward: -2.5800000922754407, Epsilon: 0.10\n",
      "Episode 475, Total Reward: -1.8350000632926822, Epsilon: 0.10\n",
      "Episode 476, Total Reward: -2.0450000716373324, Epsilon: 0.10\n",
      "Episode 477, Total Reward: -2.6600000970065594, Epsilon: 0.10\n",
      "Episode 478, Total Reward: -2.930000110529363, Epsilon: 0.10\n",
      "Episode 479, Total Reward: -2.6600000970065594, Epsilon: 0.10\n",
      "Episode 480, Total Reward: -2.810000100173056, Epsilon: 0.10\n",
      "Episode 481, Total Reward: -2.0250000655651093, Epsilon: 0.10\n",
      "Episode 482, Total Reward: -1.8350000632926822, Epsilon: 0.10\n",
      "Episode 483, Total Reward: -2.070000071078539, Epsilon: 0.10\n",
      "Episode 484, Total Reward: -1.935000067576766, Epsilon: 0.10\n",
      "Episode 485, Total Reward: -2.090000070631504, Epsilon: 0.10\n",
      "Episode 486, Total Reward: -4.195000160485506, Epsilon: 0.10\n",
      "Episode 487, Total Reward: -2.8800001051276922, Epsilon: 0.10\n",
      "Episode 488, Total Reward: -1.4850000450387597, Epsilon: 0.10\n",
      "Episode 489, Total Reward: -3.355000127106905, Epsilon: 0.10\n",
      "Episode 490, Total Reward: -3.4500001249834895, Epsilon: 0.10\n",
      "Episode 491, Total Reward: -1.850000062957406, Epsilon: 0.10\n",
      "Episode 492, Total Reward: -4.380000162869692, Epsilon: 0.10\n",
      "Episode 493, Total Reward: -2.2450000802055, Epsilon: 0.10\n",
      "Episode 494, Total Reward: -3.3650001268833876, Epsilon: 0.10\n",
      "Episode 495, Total Reward: -3.095000113360584, Epsilon: 0.10\n",
      "Episode 496, Total Reward: -2.3550000842660666, Epsilon: 0.10\n",
      "Episode 497, Total Reward: -4.38000016938895, Epsilon: 0.10\n",
      "Episode 498, Total Reward: -3.6550001399591565, Epsilon: 0.10\n",
      "Episode 499, Total Reward: -3.4650001311674714, Epsilon: 0.10\n"
     ]
    }
   ],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=\"rgb-array\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "observation_shape = env.observation_space(\"red_0\").shape\n",
    "action_shape = env.action_space(\"red_0\").n\n",
    "\n",
    "red_agent = DQNAgent(observation_shape, action_shape, device=device)\n",
    "blue_agent = PretrainedAgent(observation_shape, action_shape, device=device)\n",
    "\n",
    "train_dqn(env, red_agent, blue_agent, episodes=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T03:49:18.959559Z",
     "iopub.status.busy": "2024-12-08T03:49:18.959162Z",
     "iopub.status.idle": "2024-12-08T03:49:18.964326Z",
     "shell.execute_reply": "2024-12-08T03:49:18.963382Z",
     "shell.execute_reply.started": "2024-12-08T03:49:18.959526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    \"\"\"\n",
    "    Save the PyTorch model to a .pt file.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to save.\n",
    "        file_path (str): The file path to save the model to.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f\"Model saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T03:49:48.157194Z",
     "iopub.status.busy": "2024-12-08T03:49:48.156831Z",
     "iopub.status.idle": "2024-12-08T03:49:48.164610Z",
     "shell.execute_reply": "2024-12-08T03:49:48.163757Z",
     "shell.execute_reply.started": "2024-12-08T03:49:48.157160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to q_net_trained.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(red_agent.q_net, \"q_net_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 186056,
     "modelInstanceId": 163700,
     "sourceId": 192015,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
