{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":192015,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":163700,"modelId":186056}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q git+https://github.com/Farama-Foundation/MAgent2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:47:40.616669Z","iopub.execute_input":"2024-12-08T00:47:40.617234Z","iopub.status.idle":"2024-12-08T00:48:15.177048Z","shell.execute_reply.started":"2024-12-08T00:47:40.617197Z","shell.execute_reply":"2024-12-08T00:48:15.175842Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport numpy as np\nfrom collections import deque\nimport random\nimport torch.nn as nn\nfrom magent2.environments import battle_v4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:44.380859Z","iopub.execute_input":"2024-12-08T00:48:44.381725Z","iopub.status.idle":"2024-12-08T00:48:44.387582Z","shell.execute_reply.started":"2024-12-08T00:48:44.381676Z","shell.execute_reply":"2024-12-08T00:48:44.386600Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    def __init__(self, observation_shape, action_shape):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n            nn.ReLU(),\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n            nn.ReLU(),\n        )\n        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n        dummy_output = self.cnn(dummy_input)\n        flatten_dim = dummy_output.view(-1).shape[0]\n        self.network = nn.Sequential(\n            nn.Linear(flatten_dim, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, action_shape),\n        )\n\n    def forward(self, x):\n        assert len(x.shape) >= 3, \"only support magent input observation\"\n        x = self.cnn(x)\n        if len(x.shape) == 3:\n            batchsize = 1\n        else:\n            batchsize = x.shape[0]\n        x = x.reshape(batchsize, -1)\n        return self.network(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:44.965268Z","iopub.execute_input":"2024-12-08T00:48:44.965935Z","iopub.status.idle":"2024-12-08T00:48:44.972211Z","shell.execute_reply.started":"2024-12-08T00:48:44.965903Z","shell.execute_reply":"2024-12-08T00:48:44.971284Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class RandomAgent:\n    def __init__(self, action_space):\n        self.action_space = action_space\n\n    def get_action(self, observation):\n        return self.action_space.sample()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:45.490099Z","iopub.execute_input":"2024-12-08T00:48:45.490423Z","iopub.status.idle":"2024-12-08T00:48:45.494793Z","shell.execute_reply.started":"2024-12-08T00:48:45.490397Z","shell.execute_reply":"2024-12-08T00:48:45.493881Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class PretrainedAgent:\n    def __init__(self, n_observation, n_actions, device=\"cpu\"):\n        self.device = torch.device(device)\n        self.qnetwork = QNetwork(n_observation, n_actions).to(self.device)\n\n        self.qnetwork.load_state_dict(\n            torch.load(\"/kaggle/input/pretrained/pytorch/default/1/red.pt\", weights_only=True, map_location=self.device)\n        )\n\n    def get_action(self, observation):\n        observation = (\n            torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(self.device)\n        )\n        with torch.no_grad():\n            q_values = self.qnetwork(observation)\n        action = torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n        return action\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:46.027141Z","iopub.execute_input":"2024-12-08T00:48:46.027502Z","iopub.status.idle":"2024-12-08T00:48:46.033533Z","shell.execute_reply.started":"2024-12-08T00:48:46.027470Z","shell.execute_reply":"2024-12-08T00:48:46.032570Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, observation_shape, action_shape, buffer_size=10000, batch_size=64, lr=1e-3, gamma=0.6, device=\"cpu\"):\n        self.device = torch.device(device)\n        self.q_net = QNetwork(observation_shape, action_shape).float().to(self.device)\n        self.target_net = QNetwork(observation_shape, action_shape).float().to(self.device)\n        self.target_net.load_state_dict(self.q_net.state_dict())\n        self.target_net.eval()\n\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n        self.replay_buffer = ReplayBuffer(buffer_size)\n        self.batch_size = batch_size\n        self.gamma = gamma\n        self.action_shape = action_shape\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.1\n\n    def get_action(self, observation):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(self.action_shape)\n        else:\n            state_tensor = torch.FloatTensor(observation).unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n            with torch.no_grad():\n                return self.q_net(state_tensor).argmax().item()\n\n    def train(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return\n\n        # Sample the replay buffer\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n\n        # Filter out terminal states (actions == None)\n        valid_indices = [i for i in range(len(actions)) if actions[i] is not None]\n        if not valid_indices:  # If no valid samples, return\n            return\n\n        states = np.array([states[i] for i in valid_indices])\n        actions = np.array([actions[i] for i in valid_indices])\n        rewards = np.array([rewards[i] for i in valid_indices])\n        next_states = np.array([next_states[i] for i in valid_indices])\n        dones = np.array([dones[i] for i in valid_indices])\n\n        # Convert to tensors and move to device\n        states = torch.FloatTensor(states).permute(0, 3, 1, 2).to(self.device)\n        next_states = torch.FloatTensor(next_states).permute(0, 3, 1, 2).to(self.device)\n        actions = torch.tensor(actions).unsqueeze(1).to(self.device)\n        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n\n        # Compute Q-values and targets\n        q_values = self.q_net(states).gather(1, actions)\n        with torch.no_grad():\n            next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n\n        # Compute loss and update the network\n        loss = nn.MSELoss()(q_values, target_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n    def update_target_network(self):\n        self.target_net.load_state_dict(self.q_net.state_dict())\n\n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:46.630950Z","iopub.execute_input":"2024-12-08T00:48:46.631800Z","iopub.status.idle":"2024-12-08T00:48:46.643962Z","shell.execute_reply.started":"2024-12-08T00:48:46.631765Z","shell.execute_reply":"2024-12-08T00:48:46.643077Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Replay Buffer\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = zip(*batch)\n        return (np.stack(state), np.array(action), np.array(reward), \n                np.stack(next_state), np.array(done))\n    def __len__(self):\n        return len(self.buffer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:47.610789Z","iopub.execute_input":"2024-12-08T00:48:47.611178Z","iopub.status.idle":"2024-12-08T00:48:47.617035Z","shell.execute_reply.started":"2024-12-08T00:48:47.611146Z","shell.execute_reply":"2024-12-08T00:48:47.616081Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_dqn(env, red_agent: DQNAgent, blue_agent: RandomAgent, episodes=500, target_update_freq=5):\n    total_rewards = []\n\n    for episode in range(episodes):\n        env.reset()\n        total_reward = 0\n        for agent in env.agent_iter():\n            observation, reward, termination, truncation, _ = env.last()\n            if termination or truncation:\n                action = None\n            else:\n                if agent == \"red_0\":\n                    action = red_agent.get_action(observation)\n                else:  # \"blue_0\"\n                    action = blue_agent.get_action(observation)\n\n            env.step(action)\n            if agent == \"red_0\":\n                red_agent.replay_buffer.push(\n                    observation, action, reward, env.last()[0], termination or truncation\n                )\n                total_reward += reward\n            red_agent.train()\n\n        red_agent.decay_epsilon()\n        if episode % target_update_freq == 0:\n            red_agent.update_target_network()\n\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {red_agent.epsilon:.2f}\")\n\n    env.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:48:48.316401Z","iopub.execute_input":"2024-12-08T00:48:48.316767Z","iopub.status.idle":"2024-12-08T00:48:48.323591Z","shell.execute_reply.started":"2024-12-08T00:48:48.316736Z","shell.execute_reply":"2024-12-08T00:48:48.322816Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"env = battle_v4.env(map_size=45, render_mode=\"rgb-array\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nobservation_shape = env.observation_space(\"red_0\").shape\naction_shape = env.action_space(\"red_0\").n\n\nred_agent = DQNAgent(observation_shape, action_shape, device=device)\nblue_agent = PretrainedAgent(observation_shape, action_shape, device=device)\n\ntrain_dqn(env, red_agent, blue_agent, episodes=500)\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2024-12-08T03:47:37.771746Z","shell.execute_reply.started":"2024-12-08T00:48:54.587436Z","shell.execute_reply":"2024-12-08T03:47:37.770768Z"}},"outputs":[{"name":"stdout","text":"Episode 444, Total Reward: -2.33000008482486, Epsilon: 0.11\nEpisode 445, Total Reward: -3.0350001147016883, Epsilon: 0.11\nEpisode 446, Total Reward: -3.905000147409737, Epsilon: 0.11\nEpisode 447, Total Reward: -3.700000138953328, Epsilon: 0.11\nEpisode 448, Total Reward: -2.6500000907108188, Epsilon: 0.11\nEpisode 449, Total Reward: -2.4250000827014446, Epsilon: 0.10\nEpisode 450, Total Reward: -1.3650000412017107, Epsilon: 0.10\nEpisode 451, Total Reward: -3.150000118650496, Epsilon: 0.10\nEpisode 452, Total Reward: -2.9500001100823283, Epsilon: 0.10\nEpisode 453, Total Reward: -2.5450000930577517, Epsilon: 0.10\nEpisode 454, Total Reward: -2.5550000928342342, Epsilon: 0.10\nEpisode 455, Total Reward: -2.790000100620091, Epsilon: 0.10\nEpisode 456, Total Reward: -3.7100001387298107, Epsilon: 0.10\nEpisode 457, Total Reward: -6.785000259056687, Epsilon: 0.10\nEpisode 458, Total Reward: -1.7550000585615635, Epsilon: 0.10\nEpisode 459, Total Reward: -3.1950001176446676, Epsilon: 0.10\nEpisode 460, Total Reward: -1.970000066794455, Epsilon: 0.10\nEpisode 461, Total Reward: -3.085000113584101, Epsilon: 0.10\nEpisode 462, Total Reward: -2.4400000888854265, Epsilon: 0.10\nEpisode 463, Total Reward: -3.2800001222640276, Epsilon: 0.10\nEpisode 464, Total Reward: -3.3100001206621528, Epsilon: 0.10\nEpisode 465, Total Reward: -2.020000072196126, Epsilon: 0.10\nEpisode 466, Total Reward: -4.405000162310898, Epsilon: 0.10\nEpisode 467, Total Reward: -2.570000092498958, Epsilon: 0.10\nEpisode 468, Total Reward: -2.650000097230077, Epsilon: 0.10\nEpisode 469, Total Reward: -1.9550000671297312, Epsilon: 0.10\nEpisode 470, Total Reward: -3.370000126771629, Epsilon: 0.10\nEpisode 471, Total Reward: -2.2850000793114305, Epsilon: 0.10\nEpisode 472, Total Reward: -2.365000084042549, Epsilon: 0.10\nEpisode 473, Total Reward: -2.2450000802055, Epsilon: 0.10\nEpisode 474, Total Reward: -2.5800000922754407, Epsilon: 0.10\nEpisode 475, Total Reward: -1.8350000632926822, Epsilon: 0.10\nEpisode 476, Total Reward: -2.0450000716373324, Epsilon: 0.10\nEpisode 477, Total Reward: -2.6600000970065594, Epsilon: 0.10\nEpisode 478, Total Reward: -2.930000110529363, Epsilon: 0.10\nEpisode 479, Total Reward: -2.6600000970065594, Epsilon: 0.10\nEpisode 480, Total Reward: -2.810000100173056, Epsilon: 0.10\nEpisode 481, Total Reward: -2.0250000655651093, Epsilon: 0.10\nEpisode 482, Total Reward: -1.8350000632926822, Epsilon: 0.10\nEpisode 483, Total Reward: -2.070000071078539, Epsilon: 0.10\nEpisode 484, Total Reward: -1.935000067576766, Epsilon: 0.10\nEpisode 485, Total Reward: -2.090000070631504, Epsilon: 0.10\nEpisode 486, Total Reward: -4.195000160485506, Epsilon: 0.10\nEpisode 487, Total Reward: -2.8800001051276922, Epsilon: 0.10\nEpisode 488, Total Reward: -1.4850000450387597, Epsilon: 0.10\nEpisode 489, Total Reward: -3.355000127106905, Epsilon: 0.10\nEpisode 490, Total Reward: -3.4500001249834895, Epsilon: 0.10\nEpisode 491, Total Reward: -1.850000062957406, Epsilon: 0.10\nEpisode 492, Total Reward: -4.380000162869692, Epsilon: 0.10\nEpisode 493, Total Reward: -2.2450000802055, Epsilon: 0.10\nEpisode 494, Total Reward: -3.3650001268833876, Epsilon: 0.10\nEpisode 495, Total Reward: -3.095000113360584, Epsilon: 0.10\nEpisode 496, Total Reward: -2.3550000842660666, Epsilon: 0.10\nEpisode 497, Total Reward: -4.38000016938895, Epsilon: 0.10\nEpisode 498, Total Reward: -3.6550001399591565, Epsilon: 0.10\nEpisode 499, Total Reward: -3.4650001311674714, Epsilon: 0.10\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def save_model(model, file_path):\n    \"\"\"\n    Save the PyTorch model to a .pt file.\n    \n    Args:\n        model (nn.Module): The model to save.\n        file_path (str): The file path to save the model to.\n    \"\"\"\n    torch.save(model.state_dict(), file_path)\n    print(f\"Model saved to {file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:49:18.959162Z","iopub.execute_input":"2024-12-08T03:49:18.959559Z","iopub.status.idle":"2024-12-08T03:49:18.964326Z","shell.execute_reply.started":"2024-12-08T03:49:18.959526Z","shell.execute_reply":"2024-12-08T03:49:18.963382Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"save_model(red_agent.q_net, \"q_net_trained.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:49:48.156831Z","iopub.execute_input":"2024-12-08T03:49:48.157194Z","iopub.status.idle":"2024-12-08T03:49:48.164610Z","shell.execute_reply.started":"2024-12-08T03:49:48.157160Z","shell.execute_reply":"2024-12-08T03:49:48.163757Z"}},"outputs":[{"name":"stdout","text":"Model saved to q_net_trained.pt\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}