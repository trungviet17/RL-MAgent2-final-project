{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:30:42.682109Z",
     "iopub.status.busy": "2024-12-16T16:30:42.681790Z",
     "iopub.status.idle": "2024-12-16T16:31:15.814408Z",
     "shell.execute_reply": "2024-12-16T16:31:15.813461Z",
     "shell.execute_reply.started": "2024-12-16T16:30:42.682079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:40.587827Z",
     "iopub.status.busy": "2024-12-16T16:31:40.587461Z",
     "iopub.status.idle": "2024-12-16T16:31:40.593289Z",
     "shell.execute_reply": "2024-12-16T16:31:40.592460Z",
     "shell.execute_reply.started": "2024-12-16T16:31:40.587795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from magent2.environments import battle_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiến trúc mạng Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:18.989271Z",
     "iopub.status.busy": "2024-12-16T16:31:18.988827Z",
     "iopub.status.idle": "2024-12-16T16:31:18.996364Z",
     "shell.execute_reply": "2024-12-16T16:31:18.995544Z",
     "shell.execute_reply.started": "2024-12-16T16:31:18.989233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PretrainedQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, action_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:18.998187Z",
     "iopub.status.busy": "2024-12-16T16:31:18.997930Z",
     "iopub.status.idle": "2024-12-16T16:31:19.010079Z",
     "shell.execute_reply": "2024-12-16T16:31:19.009245Z",
     "shell.execute_reply.started": "2024-12-16T16:31:18.998163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Final_QNets(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            # nn.LayerNorm(120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            # nn.LayerNorm(84),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_layer = nn.Linear(84, action_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        x = self.network(x)\n",
    "        self.last_latent = x\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:19.011151Z",
     "iopub.status.busy": "2024-12-16T16:31:19.010919Z",
     "iopub.status.idle": "2024-12-16T16:31:19.023467Z",
     "shell.execute_reply": "2024-12-16T16:31:19.022702Z",
     "shell.execute_reply.started": "2024-12-16T16:31:19.011127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN Feature Extractor with Reduced Parameters\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], 16, kernel_size=3, padding=1),  # Reduced filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # Reduced filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        # Adaptive Pooling with Smaller Output\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))  # Smaller output size\n",
    "\n",
    "        # Calculate the flattened dimension\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1).unsqueeze(0)\n",
    "        dummy_output = self.adaptive_pool(self.cnn(dummy_input))\n",
    "        flatten_dim = dummy_output.reshape(-1).shape[0]\n",
    "\n",
    "        # Fully Connected Layers with Reduced Parameters\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 128),  # Reduced size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),  # Reduced size\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final Layer\n",
    "        self.last_layer = nn.Linear(64, action_shape)  # Match action shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, C, H, W)\n",
    "        assert len(x.shape) == 4, \"Input tensor must be 4D (batch_size, C, H, W)\"\n",
    "        \n",
    "        # Pass through CNN\n",
    "        x = self.cnn(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Flatten the features\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        # Pass through Fully Connected Layers\n",
    "        x = self.network(x)\n",
    "\n",
    "        # Output action values\n",
    "        return self.last_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:19.024897Z",
     "iopub.status.busy": "2024-12-16T16:31:19.024481Z",
     "iopub.status.idle": "2024-12-16T16:31:19.132260Z",
     "shell.execute_reply": "2024-12-16T16:31:19.131402Z",
     "shell.execute_reply.started": "2024-12-16T16:31:19.024860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 31509\n"
     ]
    }
   ],
   "source": [
    "input_shape = ( 13, 13,5)  \n",
    "num_actions = 21\n",
    "\n",
    "model = MyQNetwork(input_shape, num_actions)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:48.968093Z",
     "iopub.status.busy": "2024-12-16T16:31:48.967453Z",
     "iopub.status.idle": "2024-12-16T16:31:48.975010Z",
     "shell.execute_reply": "2024-12-16T16:31:48.974129Z",
     "shell.execute_reply.started": "2024-12-16T16:31:48.968059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "class ReplayBuffer(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return (np.stack(state), np.array(action), np.array(reward), \n",
    "                np.stack(next_state), np.array(done))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        state, action, reward, next_state, done = self.buffer[idx]\n",
    "        return (\n",
    "            torch.tensor(state), \n",
    "            torch.tensor(action), \n",
    "            torch.tensor(reward, dtype = torch.float),\n",
    "            torch.tensor(next_state), \n",
    "            torch.tensor(done, dtype = torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:51.444456Z",
     "iopub.status.busy": "2024-12-16T16:31:51.444116Z",
     "iopub.status.idle": "2024-12-16T16:31:51.449231Z",
     "shell.execute_reply": "2024-12-16T16:31:51.448256Z",
     "shell.execute_reply.started": "2024-12-16T16:31:51.444424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.n_action = action_space\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        return torch.randint(0, self.n_action, (1,)).item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:52.483991Z",
     "iopub.status.busy": "2024-12-16T16:31:52.483379Z",
     "iopub.status.idle": "2024-12-16T16:31:52.490440Z",
     "shell.execute_reply": "2024-12-16T16:31:52.489534Z",
     "shell.execute_reply.started": "2024-12-16T16:31:52.483955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PretrainedAgent:\n",
    "    def __init__(self, n_observation, n_actions, device=\"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.qnetwork = PretrainedQNetwork(n_observation, n_actions).to(self.device)\n",
    "        self.n_action = n_actions\n",
    "        self.qnetwork.load_state_dict(\n",
    "            torch.load(\"/kaggle/input/pretrained/pytorch/default/1/red.pt\", weights_only=True, map_location=self.device)\n",
    "        )\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        if np.random.rand() < 0.1:\n",
    "            return np.random.randint(self.n_action)\n",
    "        else: \n",
    "            observation = (\n",
    "                torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(self.device)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                q_values = self.qnetwork(observation)\n",
    "            action = torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:53.832722Z",
     "iopub.status.busy": "2024-12-16T16:31:53.832001Z",
     "iopub.status.idle": "2024-12-16T16:31:53.838455Z",
     "shell.execute_reply": "2024-12-16T16:31:53.837634Z",
     "shell.execute_reply.started": "2024-12-16T16:31:53.832687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FinalAgent: \n",
    "    def __init__(self, n_observation, n_actions, device = \"cpu\"): \n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.final_networks = Final_QNets(n_observation, n_actions).to(self.device)\n",
    "\n",
    "        self.final_networks.load_state_dict(\n",
    "            torch.load(\"/kaggle/input/final_rl/pytorch/default/1/red_final.pt\", weights_only = True, map_location = self.device)\n",
    "        )\n",
    "\n",
    "    def get_action(self, observation): \n",
    "        observation = (\n",
    "            torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            q_values = self.final_networks(observation)\n",
    "        action = torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:31:55.106044Z",
     "iopub.status.busy": "2024-12-16T16:31:55.105238Z",
     "iopub.status.idle": "2024-12-16T16:31:55.117066Z",
     "shell.execute_reply": "2024-12-16T16:31:55.116250Z",
     "shell.execute_reply.started": "2024-12-16T16:31:55.106009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, observation_shape, action_shape, batch_size=64, lr=1e-3, gamma=0.6, device=\"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.q_net = MyQNetwork(observation_shape, action_shape).float().to(self.device)\n",
    "        self.target_net = MyQNetwork(observation_shape, action_shape).float().to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_shape = action_shape\n",
    "        self.epsilon = 0.9\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "\n",
    "    def get_action(self, observation):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_shape)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(observation).unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return self.q_net(state_tensor).argmax().item()\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        \"\"\"\n",
    "            cap nhat lai tham so mo hinh voi input dau vao \n",
    "        \"\"\"\n",
    "        self.q_net.train()\n",
    "        for obs, action, reward, next_obs, done in dataloader: \n",
    "            self.q_net.zero_grad()\n",
    "    \n",
    "            obs = obs.permute(0, 3, 1, 2).to(self.device) \n",
    "            action = action.unsqueeze(1).to(self.device)\n",
    "            reward = reward.unsqueeze(1).to(self.device)\n",
    "            next_obs = next_obs.to(self.device)\n",
    "            next_obs = next_obs.permute(0, 3, 1, 2).to(self.device)\n",
    "            done = done.unsqueeze(1).to(self.device)\n",
    "    \n",
    "            # cap nhat gia tri q \n",
    "            with torch.no_grad(): \n",
    "                target_q_values = reward + self.gamma * (1 - done) * self.target_net(next_obs).max(1, keepdim=True)[0]\n",
    "    \n",
    "            q_values = self.q_net(obs).gather(1, action)\n",
    "    \n",
    "            loss = self.loss_fn(q_values, target_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "       \n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:32:07.482479Z",
     "iopub.status.busy": "2024-12-16T16:32:07.482137Z",
     "iopub.status.idle": "2024-12-16T16:32:13.143713Z",
     "shell.execute_reply": "2024-12-16T16:32:13.142844Z",
     "shell.execute_reply.started": "2024-12-16T16:32:07.482448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheseventeengv\u001b[0m (\u001b[33mtrungviet17\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241216_163211-fjff3gmt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trungviet17/RL_TRAINING/runs/fjff3gmt' target=\"_blank\">DeepQ_complex_pretrained</a></strong> to <a href='https://wandb.ai/trungviet17/RL_TRAINING' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trungviet17/RL_TRAINING' target=\"_blank\">https://wandb.ai/trungviet17/RL_TRAINING</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trungviet17/RL_TRAINING/runs/fjff3gmt' target=\"_blank\">https://wandb.ai/trungviet17/RL_TRAINING/runs/fjff3gmt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/trungviet17/RL_TRAINING/runs/fjff3gmt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79b55f681930>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_key = user_secrets.get_secret(\"wandb-key\")\n",
    "\n",
    "wandb.login(key = wandb_key)\n",
    "\n",
    "wandb.init(project=\"RL_TRAINING\", name=\"DeepQ_complex_pretrained\", \n",
    "           config={\"epochs_num\": 100, \"opponents\": \"pretrained+random\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:32:13.146157Z",
     "iopub.status.busy": "2024-12-16T16:32:13.145789Z",
     "iopub.status.idle": "2024-12-16T16:32:13.158459Z",
     "shell.execute_reply": "2024-12-16T16:32:13.157643Z",
     "shell.execute_reply.started": "2024-12-16T16:32:13.146119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "class Trainer : \n",
    "\n",
    "    def __init__(self, env, red_agent, blue_agent, buffer, batch_size): \n",
    "        self.red_agent = red_agent\n",
    "        self.blue_agent = blue_agent\n",
    "        self.buffer = buffer \n",
    "        self.batch_size = batch_size \n",
    "        self.env = env \n",
    "\n",
    "    def update_memory(self): \n",
    "        \"\"\"\n",
    "        Tạo ra một vòng lặp lưu trữ và cập nhật dữ liệu cho từng agent \n",
    "        \"\"\"\n",
    "        self.env.reset()\n",
    "        prev_obs = {}\n",
    "        prev_actions = {}\n",
    "        red_reward = 0 \n",
    "        blue_reward = 0 \n",
    "\n",
    "        red_agents = 0 \n",
    "        blue_agents = 0 \n",
    "\n",
    "        # vong lap 1 \n",
    "        for idx, agent in enumerate(self.env.agent_iter()): \n",
    "            prev_ob, reward, termination, truncation, _ = self.env.last()\n",
    "\n",
    "            if truncation or termination: \n",
    "                prev_action = None\n",
    "                if \"red\" in agent: red_agents +=1 \n",
    "                else : blue_agents += 1\n",
    "            else: \n",
    "                if agent.split(\"_\")[0] == \"red\": \n",
    "                    prev_action =  self.red_agent.get_action(prev_ob)\n",
    "                    red_reward += reward\n",
    "                else: \n",
    "                    prev_action = self.blue_agent.get_action(prev_ob)\n",
    "                    blue_reward += reward \n",
    "    \n",
    "\n",
    "        \n",
    "            prev_obs[agent] = prev_ob \n",
    "            prev_actions[agent] = prev_action \n",
    "            self.env.step(prev_action)\n",
    "\n",
    "            if (idx + 1) % self.env.num_agents == 0: break \n",
    "\n",
    "        # vong lap 2 \n",
    "        for agent in self.env.agent_iter(): \n",
    "\n",
    "            obs, reward, termination, truncation, _ = self.env.last()\n",
    "\n",
    "            if truncation or termination: \n",
    "                action = None \n",
    "                if \"red\" in agent: red_agents +=1 \n",
    "                else : blue_agents += 1\n",
    "            else: \n",
    "                if agent.split(\"_\")[0] == \"red\" : \n",
    "                    action = self.red_agent.get_action(obs)\n",
    "                    red_reward += reward \n",
    "                \n",
    "                else: \n",
    "                    action = self.blue_agent.get_action(obs)\n",
    "                    blue_reward += reward\n",
    "                \n",
    "\n",
    "            self.env.step(action)\n",
    "\n",
    "            self.buffer.push(\n",
    "                prev_obs[agent], \n",
    "                prev_actions[agent], \n",
    "                reward, \n",
    "                obs, \n",
    "                termination \n",
    "            )\n",
    "\n",
    "\n",
    "            prev_obs[agent] = obs \n",
    "            prev_actions[agent] = action\n",
    "\n",
    "        return red_reward - blue_reward,  red_agents - blue_agents < 0 # red thắng  \n",
    "\n",
    "    def save_model (self, file_path):\n",
    "        \n",
    "        torch.save(self.red_agent.q_net.state_dict(), file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "    def train_dqn(self, episodes=500, target_update_freq=2):\n",
    "        gap_rewards = []\n",
    "\n",
    "\n",
    "        for eps in range(episodes): \n",
    "            start = time()\n",
    "            gap_reward, winner = self.update_memory()\n",
    "            dataloader = DataLoader(self.buffer, batch_size = self.batch_size, shuffle = True)\n",
    "\n",
    "            self.red_agent.train(dataloader)\n",
    "    \n",
    "            self.red_agent.decay_epsilon()\n",
    "            if eps % target_update_freq == 0:\n",
    "                self.red_agent.update_target_network()\n",
    "    \n",
    "            end = time() - start \n",
    "            wandb.log({\n",
    "                \"episode\": eps,\n",
    "                \"gap_rewards\": gap_reward,\n",
    "                \"epsilon\": red_agent.epsilon,\n",
    "                \"time\": end\n",
    "            })\n",
    "    \n",
    "            gap_rewards.append(gap_reward)\n",
    "            print(f\"Episode {eps}, Total Reward: {gap_reward}, Epsilon: {red_agent.epsilon:.2f}, Time: {end}, Winner : {winner}\")\n",
    "    \n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:32:13.438781Z",
     "iopub.status.busy": "2024-12-16T16:32:13.438412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -442.8500009244308, Epsilon: 0.90, Time: 6.613807678222656, Winner : False\n",
      "Episode 1, Total Reward: -459.4300009747967, Epsilon: 0.89, Time: 4.546820163726807, Winner : False\n",
      "Episode 2, Total Reward: -461.89000153075904, Epsilon: 0.89, Time: 5.387091398239136, Winner : False\n",
      "Episode 3, Total Reward: -322.16000114101917, Epsilon: 0.88, Time: 13.209748268127441, Winner : False\n",
      "Episode 4, Total Reward: -404.33000082708895, Epsilon: 0.88, Time: 8.30617094039917, Winner : False\n",
      "Episode 5, Total Reward: -452.6550003858283, Epsilon: 0.87, Time: 4.1059730052948, Winner : False\n",
      "Episode 6, Total Reward: -410.6699999794364, Epsilon: 0.87, Time: 6.9331769943237305, Winner : False\n",
      "Episode 7, Total Reward: -441.5150007158518, Epsilon: 0.86, Time: 4.839049816131592, Winner : False\n",
      "Episode 8, Total Reward: -396.22000042535365, Epsilon: 0.86, Time: 7.333963632583618, Winner : False\n",
      "Episode 9, Total Reward: -410.7400005105883, Epsilon: 0.86, Time: 8.232966184616089, Winner : False\n"
     ]
    }
   ],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=\"rgb_array\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "observation_shape = env.observation_space(\"red_0\").shape\n",
    "action_shape = env.action_space(\"red_0\").n\n",
    "\n",
    "red_agent = DQNAgent(observation_shape, action_shape, device=device)\n",
    "blue_agent = PretrainedAgent(n_observation = observation_shape, n_actions = action_shape, device = device)\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "trainer = Trainer(env, red_agent, blue_agent, buffer, batch_size = 64)\n",
    "trainer.train_dqn(episodes = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:05:42.230493Z",
     "iopub.status.busy": "2024-12-16T16:05:42.229983Z",
     "iopub.status.idle": "2024-12-16T16:05:42.246585Z",
     "shell.execute_reply": "2024-12-16T16:05:42.244421Z",
     "shell.execute_reply.started": "2024-12-16T16:05:42.230450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_model5.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"my_model5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:02:19.650276Z",
     "iopub.status.busy": "2024-12-16T09:02:19.649911Z",
     "iopub.status.idle": "2024-12-16T09:16:19.380837Z",
     "shell.execute_reply": "2024-12-16T09:16:19.379923Z",
     "shell.execute_reply.started": "2024-12-16T09:02:19.650242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -421.09499831404537, Epsilon: 0.60\n",
      "Episode 1, Total Reward: 452.3800273099914, Epsilon: 0.60\n",
      "Episode 2, Total Reward: 1920.02507760562, Epsilon: 0.60\n",
      "Episode 3, Total Reward: -404.4649995714426, Epsilon: 0.59\n",
      "Episode 4, Total Reward: -408.74000167287886, Epsilon: 0.59\n",
      "Episode 5, Total Reward: -326.11000062618405, Epsilon: 0.59\n",
      "Episode 6, Total Reward: -334.4300012467429, Epsilon: 0.58\n",
      "Episode 7, Total Reward: -421.1899992218241, Epsilon: 0.58\n",
      "Episode 8, Total Reward: -389.87000301200897, Epsilon: 0.58\n",
      "Episode 9, Total Reward: -337.24499892350286, Epsilon: 0.58\n",
      "Episode 10, Total Reward: -374.6600020201877, Epsilon: 0.57\n",
      "Episode 11, Total Reward: -313.0649981154129, Epsilon: 0.57\n",
      "Episode 12, Total Reward: -351.7350029973313, Epsilon: 0.57\n",
      "Episode 13, Total Reward: -313.44000261928886, Epsilon: 0.56\n",
      "Episode 14, Total Reward: -447.4950031125918, Epsilon: 0.56\n",
      "Episode 15, Total Reward: -415.5199996698648, Epsilon: 0.56\n",
      "Episode 16, Total Reward: -393.9050019849092, Epsilon: 0.56\n",
      "Episode 17, Total Reward: -358.1400028001517, Epsilon: 0.55\n",
      "Episode 18, Total Reward: -383.2749997340143, Epsilon: 0.55\n",
      "Episode 19, Total Reward: 1325.3950533019379, Epsilon: 0.55\n",
      "Episode 20, Total Reward: -405.76999785751104, Epsilon: 0.55\n",
      "Episode 21, Total Reward: -368.29500043950975, Epsilon: 0.54\n",
      "Episode 22, Total Reward: -342.29499936290085, Epsilon: 0.54\n",
      "Episode 23, Total Reward: -280.2100010002032, Epsilon: 0.54\n",
      "Episode 24, Total Reward: -349.29499882273376, Epsilon: 0.53\n",
      "Episode 25, Total Reward: -411.02000016812235, Epsilon: 0.53\n",
      "Episode 26, Total Reward: -432.16500062309206, Epsilon: 0.53\n",
      "Episode 27, Total Reward: -280.5150036746636, Epsilon: 0.53\n",
      "Episode 28, Total Reward: -321.67000352125615, Epsilon: 0.52\n",
      "Episode 29, Total Reward: -350.2349991835654, Epsilon: 0.52\n",
      "Episode 30, Total Reward: -418.2299994630739, Epsilon: 0.52\n",
      "Episode 31, Total Reward: -367.3650004733354, Epsilon: 0.52\n",
      "Episode 32, Total Reward: -385.280000439845, Epsilon: 0.51\n",
      "Episode 33, Total Reward: -380.4299999838695, Epsilon: 0.51\n",
      "Episode 34, Total Reward: -264.0099990237504, Epsilon: 0.51\n",
      "Episode 35, Total Reward: -314.30000482313335, Epsilon: 0.51\n",
      "Episode 36, Total Reward: -378.4349978417158, Epsilon: 0.50\n",
      "Episode 37, Total Reward: 1621.9950655773282, Epsilon: 0.50\n",
      "Episode 38, Total Reward: -342.3249982818961, Epsilon: 0.50\n",
      "Episode 39, Total Reward: -285.5600019292906, Epsilon: 0.50\n",
      "Episode 40, Total Reward: -253.02499950490892, Epsilon: 0.49\n",
      "Episode 41, Total Reward: -355.5849990779534, Epsilon: 0.49\n",
      "Episode 42, Total Reward: -299.62500085588545, Epsilon: 0.49\n",
      "Episode 43, Total Reward: -338.5550001934171, Epsilon: 0.49\n",
      "Episode 44, Total Reward: -387.45999865885824, Epsilon: 0.48\n",
      "Episode 45, Total Reward: -99.5500059761107, Epsilon: 0.48\n",
      "Episode 46, Total Reward: -298.6199982808903, Epsilon: 0.48\n",
      "Episode 47, Total Reward: -300.7850017165765, Epsilon: 0.48\n",
      "Episode 48, Total Reward: -347.4949985984713, Epsilon: 0.47\n",
      "Episode 49, Total Reward: -661.4750283583999, Epsilon: 0.47\n",
      "Episode 50, Total Reward: -344.9249988337979, Epsilon: 0.47\n",
      "Episode 51, Total Reward: 1648.1450666775927, Epsilon: 0.47\n",
      "Episode 52, Total Reward: -256.8949984014034, Epsilon: 0.46\n",
      "Episode 53, Total Reward: -442.1000034790486, Epsilon: 0.46\n",
      "Episode 54, Total Reward: -204.74999490845948, Epsilon: 0.46\n",
      "Episode 55, Total Reward: -446.17500088363886, Epsilon: 0.46\n",
      "Episode 56, Total Reward: -259.435003971681, Epsilon: 0.46\n",
      "Episode 57, Total Reward: -254.32500047329813, Epsilon: 0.45\n",
      "Episode 58, Total Reward: -301.1600132258609, Epsilon: 0.45\n",
      "Episode 59, Total Reward: -274.9249990982935, Epsilon: 0.45\n",
      "Episode 60, Total Reward: -246.7300042361021, Epsilon: 0.45\n",
      "Episode 61, Total Reward: -92.76500616595149, Epsilon: 0.44\n",
      "Episode 62, Total Reward: -297.1199988964945, Epsilon: 0.44\n",
      "Episode 63, Total Reward: -416.1850076811388, Epsilon: 0.44\n",
      "Episode 64, Total Reward: -910.6650384226814, Epsilon: 0.44\n",
      "Episode 65, Total Reward: -39.12999132927507, Epsilon: 0.44\n",
      "Episode 66, Total Reward: -365.09999837353826, Epsilon: 0.43\n",
      "Episode 67, Total Reward: -312.58499996736646, Epsilon: 0.43\n",
      "Episode 68, Total Reward: -96.9150057155639, Epsilon: 0.43\n",
      "Episode 69, Total Reward: -208.74999951664358, Epsilon: 0.43\n",
      "Episode 70, Total Reward: -317.1650012023747, Epsilon: 0.42\n",
      "Episode 71, Total Reward: -343.7400006391108, Epsilon: 0.42\n",
      "Episode 72, Total Reward: -207.43500127177685, Epsilon: 0.42\n",
      "Episode 73, Total Reward: -338.85999899636954, Epsilon: 0.42\n",
      "Episode 74, Total Reward: -212.90999961737543, Epsilon: 0.42\n",
      "Episode 75, Total Reward: -216.80500243604183, Epsilon: 0.41\n",
      "Episode 76, Total Reward: -362.36499793827534, Epsilon: 0.41\n",
      "Episode 77, Total Reward: -415.8200037078932, Epsilon: 0.41\n",
      "Episode 78, Total Reward: -648.8450283361599, Epsilon: 0.41\n",
      "Episode 79, Total Reward: -125.79500013589859, Epsilon: 0.41\n",
      "Episode 80, Total Reward: -81.44499543309212, Epsilon: 0.40\n",
      "Episode 81, Total Reward: -262.7199997222051, Epsilon: 0.40\n",
      "Episode 82, Total Reward: -279.49499975983053, Epsilon: 0.40\n",
      "Episode 83, Total Reward: -190.83000120334327, Epsilon: 0.40\n",
      "Episode 84, Total Reward: -312.67000045534223, Epsilon: 0.40\n",
      "Episode 85, Total Reward: -270.48499990068376, Epsilon: 0.39\n",
      "Episode 86, Total Reward: -1162.7350483760238, Epsilon: 0.39\n",
      "Episode 87, Total Reward: -252.61000014096498, Epsilon: 0.39\n",
      "Episode 88, Total Reward: 21.169987246394157, Epsilon: 0.39\n",
      "Episode 89, Total Reward: -91.1500012613833, Epsilon: 0.39\n",
      "Episode 90, Total Reward: 62.26500045508146, Epsilon: 0.38\n",
      "Episode 91, Total Reward: -295.2600021697581, Epsilon: 0.38\n",
      "Episode 92, Total Reward: -484.2350188726559, Epsilon: 0.38\n",
      "Episode 93, Total Reward: -145.210002662614, Epsilon: 0.38\n",
      "Episode 94, Total Reward: -211.7950013736263, Epsilon: 0.38\n",
      "Episode 95, Total Reward: -263.86500012874603, Epsilon: 0.37\n",
      "Episode 96, Total Reward: -74.87000558711588, Epsilon: 0.37\n",
      "Episode 97, Total Reward: 925.1800377909094, Epsilon: 0.37\n",
      "Episode 98, Total Reward: -252.61499996855855, Epsilon: 0.37\n",
      "Episode 99, Total Reward: -336.42000379785895, Epsilon: 0.37\n"
     ]
    }
   ],
   "source": [
    "blue_agent = PretrainedAgent(n_observation = observation_shape, n_actions = action_shape)\n",
    "\n",
    "trainer = Trainer(env, red_agent, blue_agent, buffer, batch_size = 64)\n",
    "trainer.train_dqn(episodes = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:19:38.734617Z",
     "iopub.status.busy": "2024-12-16T09:19:38.734285Z",
     "iopub.status.idle": "2024-12-16T09:19:38.745623Z",
     "shell.execute_reply": "2024-12-16T09:19:38.744882Z",
     "shell.execute_reply.started": "2024-12-16T09:19:38.734588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_model2.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"my_model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:11:22.629335Z",
     "iopub.status.busy": "2024-12-16T16:11:22.628915Z",
     "iopub.status.idle": "2024-12-16T16:27:57.692739Z",
     "shell.execute_reply": "2024-12-16T16:27:57.691156Z",
     "shell.execute_reply.started": "2024-12-16T16:11:22.629285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -835.3250285536051, Epsilon: 0.33, Time: 45.99792528152466, Winner : False\n",
      "Episode 1, Total Reward: -733.5650315135717, Epsilon: 0.33, Time: 47.74588489532471, Winner : False\n",
      "Episode 2, Total Reward: -956.6750405393541, Epsilon: 0.32, Time: 62.174410343170166, Winner : False\n",
      "Episode 3, Total Reward: -163.13499972503632, Epsilon: 0.32, Time: 21.861814737319946, Winner : False\n",
      "Episode 4, Total Reward: -37.12999481894076, Epsilon: 0.32, Time: 23.885837078094482, Winner : False\n",
      "Episode 5, Total Reward: -251.18500122893602, Epsilon: 0.32, Time: 16.812191247940063, Winner : False\n",
      "Episode 6, Total Reward: 49.85500467475504, Epsilon: 0.32, Time: 42.093571186065674, Winner : False\n",
      "Episode 7, Total Reward: -238.65000720508397, Epsilon: 0.32, Time: 21.857694387435913, Winner : False\n",
      "Episode 8, Total Reward: -526.9850208768621, Epsilon: 0.31, Time: 36.23681354522705, Winner : False\n",
      "Episode 9, Total Reward: -496.96502331178635, Epsilon: 0.31, Time: 47.10835075378418, Winner : False\n",
      "Episode 10, Total Reward: -707.0200307108462, Epsilon: 0.31, Time: 59.01021122932434, Winner : False\n",
      "Episode 11, Total Reward: -626.5000320393592, Epsilon: 0.31, Time: 74.11718726158142, Winner : False\n",
      "Episode 12, Total Reward: -139.4800061704591, Epsilon: 0.31, Time: 40.45267724990845, Winner : False\n",
      "Episode 13, Total Reward: -282.16500596888363, Epsilon: 0.31, Time: 20.182392597198486, Winner : False\n",
      "Episode 14, Total Reward: -694.6750286072493, Epsilon: 0.30, Time: 36.009770154953, Winner : False\n",
      "Episode 15, Total Reward: -366.5000138692558, Epsilon: 0.30, Time: 29.479207038879395, Winner : False\n",
      "Episode 16, Total Reward: -499.9350229874253, Epsilon: 0.30, Time: 61.02124309539795, Winner : False\n",
      "Episode 17, Total Reward: -508.7300212942064, Epsilon: 0.30, Time: 68.96795105934143, Winner : False\n",
      "Episode 18, Total Reward: -102.08500889223069, Epsilon: 0.30, Time: 16.66159725189209, Winner : False\n",
      "Episode 19, Total Reward: -539.4000261090696, Epsilon: 0.30, Time: 77.72169423103333, Winner : False\n",
      "Episode 20, Total Reward: -200.6000142134726, Epsilon: 0.30, Time: 25.83086395263672, Winner : False\n",
      "Episode 21, Total Reward: -360.4050132688135, Epsilon: 0.29, Time: 29.47809863090515, Winner : False\n",
      "Episode 22, Total Reward: -338.59001367446035, Epsilon: 0.29, Time: 33.27275252342224, Winner : False\n",
      "Episode 23, Total Reward: 98.55999309569597, Epsilon: 0.29, Time: 13.43752908706665, Winner : False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m blue_agent \u001b[38;5;241m=\u001b[39m FinalAgent(n_observation \u001b[38;5;241m=\u001b[39m observation_shape, n_actions \u001b[38;5;241m=\u001b[39m action_shape)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(env, red_agent, blue_agent, buffer, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 98\u001b[0m, in \u001b[0;36mTrainer.train_dqn\u001b[0;34m(self, episodes, target_update_freq)\u001b[0m\n\u001b[1;32m     95\u001b[0m gap_reward, winner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_memory()\n\u001b[1;32m     96\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mred_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mred_agent\u001b[38;5;241m.\u001b[39mdecay_epsilon()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eps \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    cap nhat lai tham so mo hinh voi input dau vao \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs, action, reward, next_obs, done \u001b[38;5;129;01min\u001b[39;00m dataloader: \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "blue_agent = FinalAgent(n_observation = observation_shape, n_actions = action_shape)\n",
    "\n",
    "trainer = Trainer(env, red_agent, blue_agent, buffer, batch_size = 64)\n",
    "trainer.train_dqn(episodes = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T10:08:34.248855Z",
     "iopub.status.busy": "2024-12-16T10:08:34.248015Z",
     "iopub.status.idle": "2024-12-16T10:08:34.259045Z",
     "shell.execute_reply": "2024-12-16T10:08:34.258243Z",
     "shell.execute_reply.started": "2024-12-16T10:08:34.248819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_model3.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"my_model3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Mix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, action_size):\n",
    "        super(AgentQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(np.prod(obs_shape), 64)\n",
    "        self.gru = nn.GRU(64, 64, batch_first=True)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, obs, hidden_state):\n",
    "        obs = obs.flatten(start_dim=1) \n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x, hidden_state = self.gru(x.unsqueeze(1), hidden_state)\n",
    "        q_values = self.fc2(x.squeeze(1))\n",
    "        return q_values, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MixingNetwork(nn.Module):\n",
    "    def __init__(self, num_agents, state_shape):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.state_fc1 = nn.Linear(np.prod(state_shape), 64)\n",
    "        self.qmix_fc = nn.Linear(64, num_agents)\n",
    "\n",
    "    def forward(self, q_values, state):\n",
    "        state = state.flatten(start_dim=1) \n",
    "        state_features = torch.relu(self.state_fc1(state))\n",
    "        weights = self.qmix_fc(state_features).unsqueeze(-1)  \n",
    "        q_total = torch.sum(q_values * weights, dim=1)\n",
    "        return q_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        state, observation, action, reward, next_state, done = transition\n",
    "        action = action if action is not None else -1  # Replace None with a sentinel value\n",
    "        self.buffer.append((state, observation, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, observations, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (torch.tensor(states, dtype=torch.float32),\n",
    "                torch.tensor(observations, dtype=torch.float32),\n",
    "                torch.tensor(actions, dtype=torch.long),\n",
    "                torch.tensor(rewards, dtype=torch.float32),\n",
    "                torch.tensor(next_states, dtype=torch.float32),\n",
    "                torch.tensor(dones, dtype=torch.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 1000\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.05\n",
    "\n",
    "batch_size = 64\n",
    "buffer_capacity = 10000\n",
    "update_target_every = 100\n",
    "\n",
    "# Agent and mixing network setup\n",
    "obs_shape = (13, 13, 5)\n",
    "state_shape = (45, 45, 5)\n",
    "action_size = 21\n",
    "num_agents = 81\n",
    "\n",
    "# Initialize shared Q-network, target network, and mixing network\n",
    "shared_q_network = AgentQNetwork(obs_shape, action_size)\n",
    "target_q_network = AgentQNetwork(obs_shape, action_size)\n",
    "target_q_network.load_state_dict(shared_q_network.state_dict())\n",
    "mixing_network = MixingNetwork(num_agents, state_shape)\n",
    "mixing_target_network = MixingNetwork(num_agents, state_shape)\n",
    "mixing_target_network.load_state_dict(mixing_network.state_dict())\n",
    "\n",
    "# Optimizers\n",
    "optimizer = optim.Adam(shared_q_network.parameters(), lr=learning_rate)\n",
    "mix_optimizer = optim.Adam(mixing_network.parameters(), lr=learning_rate)\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    hidden_states = {agent: None for agent in env.agents}  # Initialize hidden states for GRU\n",
    "    done = False\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "\n",
    "    agent_observations = {}\n",
    "    agent_rewards = {}\n",
    "    agent_dones = {}\n",
    "    count = 0 \n",
    "    red_count = 0 \n",
    "    blue_count = 0 \n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, _ = env.last()\n",
    "        \n",
    "        if hidden_states[agent] is None:\n",
    "            hidden_states[agent] = torch.zeros((1, 1, 64))\n",
    "\n",
    "        if termination or truncation:\n",
    "            if agent.split(\"_\")[0] == 'red' : red_count += 1\n",
    "            else: blue_count += 1 \n",
    "            action = None\n",
    "            print(f\"Blue: {blue_count}, Red: {red_count}\")\n",
    "        else:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(action_size)\n",
    "            else:\n",
    "                obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "                q_values, hidden_states[agent] = shared_q_network(obs_tensor, hidden_states[agent])\n",
    "                action = torch.argmax(q_values).item()\n",
    "        env.step(action)\n",
    "        \n",
    "\n",
    "        agent_observations[agent] = observation\n",
    "        agent_rewards[agent] = reward\n",
    "        agent_dones[agent] = termination or truncation\n",
    "\n",
    "        replay_buffer.push((env.state(), observation, action, reward, env.state(), termination or truncation))\n",
    "        if red_count == 81 or blue_count == 81 : break \n",
    "\n",
    "    # Sample from replay buffer and update networks\n",
    "    if len(replay_buffer) >= batch_size:\n",
    "        states, observations, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Q-value prediction\n",
    "        q_values, _ = shared_q_network(observations, None)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-value prediction\n",
    "        with torch.no_grad():\n",
    "            next_q_values, _ = target_q_network(observations, None)\n",
    "            next_q_values = next_q_values.max(1)[0]\n",
    "            target_q_values = rewards + gamma * (1 - dones) * next_q_values\n",
    "\n",
    "        loss = torch.mean((q_values - target_q_values) ** 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update mixing network\n",
    "    q_total = mixing_network(q_values, states)\n",
    "    with torch.no_grad():\n",
    "        next_q_total = mixing_target_network(target_q_values.unsqueeze(1), next_states)\n",
    "    mix_loss = torch.mean((q_total - next_q_total) ** 2)\n",
    "\n",
    "    mix_optimizer.zero_grad()\n",
    "    mix_loss.backward()\n",
    "    mix_optimizer.step()\n",
    "\n",
    "    # Update target networks\n",
    "    if episode % update_target_every == 0:\n",
    "        target_q_network.load_state_dict(shared_q_network.state_dict())\n",
    "        mixing_target_network.load_state_dict(mixing_network.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 186056,
     "modelInstanceId": 163700,
     "sourceId": 192015,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 192823,
     "modelInstanceId": 170510,
     "sourceId": 199886,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
