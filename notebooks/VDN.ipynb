{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUeq-RARIw8V",
        "outputId": "30910c85-9d51-48dc-b730-fac742aac775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-xjpbpmle\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-xjpbpmle\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Requirement already satisfied: pettingzoo>=1.23.1 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.24.3)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "# !pip install git+https://github.com/Farama-Foundation/MAgent2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJhtgdi9Iw8X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from magent2.environments import battle_v4\n",
        "from pettingzoo.utils import random_demo\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from collections import deque\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import imageio\n",
        "from torch.nn import MSELoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfBAFHU2Iw8X"
      },
      "outputs": [],
      "source": [
        "def save_model(model, file_path):\n",
        "    torch.save(model.state_dict(), file_path)\n",
        "    print(f\"Model saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEAduYFwIw8Y"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOUEj2D0Iw8Y"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onxHe_oXIw8Z"
      },
      "outputs": [],
      "source": [
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return (np.stack(state), np.array(action), np.array(reward),\n",
        "                np.stack(next_state), np.array(done))\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8BcJ-EqIw8c"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def get_action(self, observation):\n",
        "        return self.action_space.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_78bLnkIw8a"
      },
      "source": [
        "### VDN networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uThaKWFTIw8a"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, action_shape),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"Input shape error\"\n",
        "        x = self.cnn(x)\n",
        "        batchsize = x.shape[0] if len(x.shape) > 3 else 1\n",
        "        x = x.reshape(batchsize, -1)\n",
        "        return self.network(x)\n",
        "\n",
        "class VDN:\n",
        "    def __init__(self, observation_shape, action_shape, agents, batch_size=64, lr=1e-3, gamma=0.8, device=\"cpu\"):\n",
        "        self.device = torch.device(device)\n",
        "        self.agents = agents\n",
        "        self.q_networks = {\n",
        "            agent: QNetwork(observation_shape, action_shape).to(self.device)\n",
        "            for agent in agents\n",
        "        }\n",
        "        self.lr = lr\n",
        "        self.optimizers = {\n",
        "            agent: optim.Adam(self.q_networks[agent].parameters(), lr=self.lr)\n",
        "            for agent in agents\n",
        "        }\n",
        "        self.schedulers = {\n",
        "            agent: torch.optim.lr_scheduler.StepLR(self.optimizers[agent], step_size=10, gamma=0.9)\n",
        "            for agent in agents\n",
        "        }\n",
        "        self.target_networks = {\n",
        "            agent: QNetwork(observation_shape, action_shape).to(self.device)\n",
        "            for agent in agents\n",
        "        }\n",
        "        for agent in agents:\n",
        "            self.target_networks[agent].load_state_dict(self.q_networks[agent].state_dict())\n",
        "            self.target_networks[agent].eval()\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(capacity=10000)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.action_shape = action_shape\n",
        "        self.epsilon = 0.5\n",
        "        self.epsilon_decay = 0.9\n",
        "        self.epsilon_min = 0.05\n",
        "        self.max_grad_norm = 1.0\n",
        "\n",
        "    def get_action(self, agent, observation):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_shape)\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(observation).unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                return self.q_networks[agent](state_tensor).argmax().item()\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        states = torch.FloatTensor(states).permute(0, 3, 1, 2).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).permute(0, 3, 1, 2).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        q_values = []\n",
        "        for agent in self.agents:\n",
        "            q_value = self.q_networks[agent](states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "            q_values.append(q_value)\n",
        "        q_tot = torch.sum(torch.stack(q_values, dim=0), dim=0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = []\n",
        "            for agent in self.agents:\n",
        "                next_q_value = self.target_networks[agent](next_states).max(dim=1)[0]\n",
        "                next_q_values.append(next_q_value)\n",
        "            next_q_tot = torch.sum(torch.stack(next_q_values, dim=0), dim=0)\n",
        "            q_tot_target = rewards + self.gamma * (1 - dones) * next_q_tot\n",
        "\n",
        "        loss = torch.mean((q_tot - q_tot_target) ** 2)\n",
        "\n",
        "        for agent in self.agents:\n",
        "            self.optimizers[agent].zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        for agent in self.agents:\n",
        "            torch.nn.utils.clip_grad_norm_(self.q_networks[agent].parameters(), self.max_grad_norm)\n",
        "        for agent in self.agents:\n",
        "            self.optimizers[agent].step()\n",
        "\n",
        "        # print(f\"Loss: {loss.item()}\")\n",
        "        # if \"red_0\" in self.agents:\n",
        "        #     agent = \"red_0\"\n",
        "        #     for name, param in self.q_networks[agent].named_parameters():\n",
        "        #         if param.grad is not None:\n",
        "        #             print(f\"Gradient for {agent} -> {name}: {param.grad.abs().mean().item()}\")\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        for agent in self.agents:\n",
        "            self.target_networks[agent].load_state_dict(self.q_networks[agent].state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "muP8qMNT-_Ml",
        "outputId": "2c50d0fe-4ee6-49e2-ee65-1322cf210a74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheseventeengv\u001b[0m (\u001b[33mtrungviet17\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241221_071619-bummk21n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/trungviet17/RL_TRAINING/runs/bummk21n' target=\"_blank\">VDN</a></strong> to <a href='https://wandb.ai/trungviet17/RL_TRAINING' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/trungviet17/RL_TRAINING' target=\"_blank\">https://wandb.ai/trungviet17/RL_TRAINING</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/trungviet17/RL_TRAINING/runs/bummk21n' target=\"_blank\">https://wandb.ai/trungviet17/RL_TRAINING/runs/bummk21n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/trungviet17/RL_TRAINING/runs/bummk21n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7d6950884580>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "wandb_key = userdata.get(\"wandb-key\")\n",
        "\n",
        "wandb.login(key = wandb_key)\n",
        "\n",
        "wandb.init(project=\"RL_TRAINING\", name=\"VDN\",\n",
        "            config={\"epochs_num\": 70, \"opponents\": \"random, training with blue + red data\", \"batch_size\" : 128, \"num_agent\": 81})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bavbSC9Iw8c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def train_vdn(env, red_agents, blue_agents, episodes=70, target_update_freq=5):\n",
        "    total_rewards = []\n",
        "    red_rewards = 0\n",
        "    blue_rewards = 0\n",
        "    for episode in range(episodes):\n",
        "        kill_counts = {\"red\": 0, \"blue\": 0}\n",
        "        env.reset()\n",
        "        episode_kills = {\"red\": 0, \"blue\": 0}  # Track kills for the current episode\n",
        "        total_reward = {agent: 0 for agent in blue_agents.agents}  # Initialize rewards for each red agent\n",
        "\n",
        "        start = time.time()\n",
        "        for agent in env.agent_iter():\n",
        "            observation, reward, termination, truncation, _ = env.last()\n",
        "\n",
        "            # Handle agent termination or truncation\n",
        "            if termination or truncation:\n",
        "                env.step(None)  # Pass None explicitly for terminated agents\n",
        "                continue\n",
        "\n",
        "            team = agent.split(\"_\")[0]\n",
        "\n",
        "            if reward > 4.5:\n",
        "                episode_kills[team] += 1\n",
        "\n",
        "            # Decide action based on the agent type\n",
        "            if agent.startswith(\"blue\"):\n",
        "                # Red agent uses the VDN model to select action\n",
        "                action = blue_agents.get_action(agent, observation)\n",
        "                red_rewards += reward\n",
        "            else:\n",
        "                # Blue agent uses the random agent policy\n",
        "                action = red_agents.get_action(observation)\n",
        "                blue_rewards += reward\n",
        "\n",
        "            env.step(action)\n",
        "\n",
        "            # Update replay buffer and rewards for red agents\n",
        "            if agent.startswith(\"blue\"):\n",
        "                # next_obs = env.last()[0] if agent in env.agents else None\n",
        "                blue_agents.replay_buffer.push(\n",
        "                    observation, action, reward, env.last()[0], termination or truncation\n",
        "                )\n",
        "                total_reward[agent] += reward\n",
        "\n",
        "        # Train VDN model after each episode\n",
        "        blue_agents.update()\n",
        "\n",
        "        # Decay epsilon for exploration-exploitation balance\n",
        "        blue_agents.decay_epsilon()\n",
        "\n",
        "        # Update target networks periodically\n",
        "        if episode % target_update_freq == 0:\n",
        "            blue_agents.update_target_networks()\n",
        "\n",
        "        # Accumulate kills for this episode\n",
        "        for team in kill_counts:\n",
        "            kill_counts[team] += episode_kills[team]\n",
        "\n",
        "        # Calculate total reward for this episode\n",
        "        episode_total_reward = sum(total_reward.values())\n",
        "        total_rewards.append(episode_total_reward)\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Episode {episode}, Total Reward: {episode_total_reward}\")\n",
        "        print(f\"Episode {episode} Kills - Red: {episode_kills['red']}, Blue: {episode_kills['blue']}\")\n",
        "        wandb.log({\n",
        "                \"episode\": episode,\n",
        "                \"gap_rewards\": blue_rewards - red_rewards,\n",
        "                \"epsilon\": vdn.epsilon,\n",
        "                \"time\": time.time() - start,\n",
        "                \"red_kill\": episode_kills[\"red\"],\n",
        "                \"blue_kill\": episode_kills[\"blue\"]\n",
        "            })\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Total Kills - Red: {kill_counts['red']}, Blue: {kill_counts['blue']}\")\n",
        "    return total_rewards, kill_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmgegFKnIw8c",
        "outputId": "c2616734-62ce-4bb1-e676-c6e991e4163b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, Total Reward: -3162.0601164018735\n",
            "Episode 0 Kills - Red: 4, Blue: 12\n",
            "Episode 1, Total Reward: -3257.4851212650537\n",
            "Episode 1 Kills - Red: 3, Blue: 13\n",
            "Episode 2, Total Reward: -3195.220118932426\n",
            "Episode 2 Kills - Red: 2, Blue: 18\n",
            "Episode 3, Total Reward: -3190.8101193299517\n",
            "Episode 3 Kills - Red: 6, Blue: 17\n",
            "Episode 4, Total Reward: -3307.1401236010715\n",
            "Episode 4 Kills - Red: 2, Blue: 18\n",
            "Episode 5, Total Reward: -3315.2751254737377\n",
            "Episode 5 Kills - Red: 4, Blue: 21\n",
            "Episode 6, Total Reward: -3304.195124122314\n",
            "Episode 6 Kills - Red: 2, Blue: 22\n",
            "Episode 7, Total Reward: -3425.720127790235\n",
            "Episode 7 Kills - Red: 4, Blue: 15\n",
            "Episode 8, Total Reward: -3355.8851252188906\n",
            "Episode 8 Kills - Red: 1, Blue: 19\n",
            "Episode 9, Total Reward: -3396.815126657486\n",
            "Episode 9 Kills - Red: 3, Blue: 19\n",
            "Episode 10, Total Reward: -3440.490129268728\n",
            "Episode 10 Kills - Red: 3, Blue: 21\n",
            "Episode 11, Total Reward: -3504.320131923072\n",
            "Episode 11 Kills - Red: 2, Blue: 21\n",
            "Episode 12, Total Reward: -3678.1501383213326\n",
            "Episode 12 Kills - Red: 3, Blue: 17\n",
            "Episode 13, Total Reward: -3872.7651490336284\n",
            "Episode 13 Kills - Red: 3, Blue: 25\n",
            "Episode 14, Total Reward: -3987.3151526153088\n",
            "Episode 14 Kills - Red: 4, Blue: 23\n",
            "Episode 15, Total Reward: -3839.8151483815163\n",
            "Episode 15 Kills - Red: 4, Blue: 29\n",
            "Episode 16, Total Reward: -4034.7551588425413\n",
            "Episode 16 Kills - Red: 3, Blue: 38\n",
            "Episode 17, Total Reward: -4057.3501570988446\n",
            "Episode 17 Kills - Red: 5, Blue: 27\n",
            "Episode 18, Total Reward: -4041.745155710727\n",
            "Episode 18 Kills - Red: 2, Blue: 28\n",
            "Episode 19, Total Reward: -4114.28015985433\n",
            "Episode 19 Kills - Red: 5, Blue: 29\n",
            "Episode 20, Total Reward: -4315.100172062404\n",
            "Episode 20 Kills - Red: 2, Blue: 44\n",
            "Episode 21, Total Reward: -4212.260164102539\n",
            "Episode 21 Kills - Red: 3, Blue: 29\n",
            "Episode 22, Total Reward: -4272.250166789629\n",
            "Episode 22 Kills - Red: 2, Blue: 33\n",
            "Episode 23, Total Reward: -3902.170149588026\n",
            "Episode 23 Kills - Red: 4, Blue: 24\n",
            "Episode 24, Total Reward: -3839.39514684584\n",
            "Episode 24 Kills - Red: 0, Blue: 28\n",
            "Episode 25, Total Reward: -3852.0751460064203\n",
            "Episode 25 Kills - Red: 4, Blue: 20\n",
            "Episode 26, Total Reward: -3812.4301469316706\n",
            "Episode 26 Kills - Red: 1, Blue: 30\n",
            "Episode 27, Total Reward: -3770.945142383687\n",
            "Episode 27 Kills - Red: 2, Blue: 23\n",
            "Episode 28, Total Reward: -3666.4701394094154\n",
            "Episode 28 Kills - Red: 3, Blue: 25\n",
            "Episode 29, Total Reward: -3547.810136287473\n",
            "Episode 29 Kills - Red: 3, Blue: 32\n",
            "Episode 30, Total Reward: -3826.7451447462663\n",
            "Episode 30 Kills - Red: 1, Blue: 22\n",
            "Episode 31, Total Reward: -3710.0251395516098\n",
            "Episode 31 Kills - Red: 1, Blue: 20\n",
            "Episode 32, Total Reward: -3672.5351411318406\n",
            "Episode 32 Kills - Red: 3, Blue: 30\n",
            "Episode 33, Total Reward: -3951.9101533405483\n",
            "Episode 33 Kills - Red: 2, Blue: 33\n",
            "Episode 34, Total Reward: -3993.4401520341635\n",
            "Episode 34 Kills - Red: 2, Blue: 22\n",
            "Episode 35, Total Reward: -4089.3951577236876\n",
            "Episode 35 Kills - Red: 0, Blue: 30\n",
            "Episode 36, Total Reward: -4270.695166915655\n",
            "Episode 36 Kills - Red: 0, Blue: 36\n",
            "Episode 37, Total Reward: -4435.695172464475\n",
            "Episode 37 Kills - Red: 0, Blue: 29\n",
            "Episode 38, Total Reward: -4620.935182282701\n",
            "Episode 38 Kills - Red: 2, Blue: 33\n",
            "Episode 39, Total Reward: -4602.225181849673\n",
            "Episode 39 Kills - Red: 3, Blue: 34\n",
            "Episode 40, Total Reward: -4855.970191532746\n",
            "Episode 40 Kills - Red: 2, Blue: 32\n",
            "Episode 41, Total Reward: -4765.725188932382\n",
            "Episode 41 Kills - Red: 2, Blue: 38\n",
            "Episode 42, Total Reward: -4759.145189904608\n",
            "Episode 42 Kills - Red: 2, Blue: 43\n",
            "Episode 43, Total Reward: -4891.420192338526\n",
            "Episode 43 Kills - Red: 2, Blue: 29\n",
            "Episode 44, Total Reward: -4727.710188002326\n",
            "Episode 44 Kills - Red: 1, Blue: 40\n",
            "Episode 45, Total Reward: -4711.760185683146\n",
            "Episode 45 Kills - Red: 3, Blue: 33\n",
            "Episode 46, Total Reward: -4645.2001850269735\n",
            "Episode 46 Kills - Red: 1, Blue: 42\n",
            "Episode 47, Total Reward: -4962.560195904225\n",
            "Episode 47 Kills - Red: 2, Blue: 33\n",
            "Episode 48, Total Reward: -4889.595192625187\n",
            "Episode 48 Kills - Red: 0, Blue: 31\n",
            "Episode 49, Total Reward: -4833.190192029811\n",
            "Episode 49 Kills - Red: 1, Blue: 39\n",
            "Episode 50, Total Reward: -4826.6951902825385\n",
            "Episode 50 Kills - Red: 0, Blue: 33\n",
            "Episode 51, Total Reward: -4759.9501879950985\n",
            "Episode 51 Kills - Red: 3, Blue: 34\n",
            "Episode 52, Total Reward: -4719.620187486522\n",
            "Episode 52 Kills - Red: 2, Blue: 38\n",
            "Episode 53, Total Reward: -4786.550188005902\n",
            "Episode 53 Kills - Red: 1, Blue: 31\n",
            "Episode 54, Total Reward: -4889.095191346481\n",
            "Episode 54 Kills - Red: 0, Blue: 27\n",
            "Episode 55, Total Reward: -4674.210183446296\n",
            "Episode 55 Kills - Red: 4, Blue: 30\n",
            "Episode 56, Total Reward: -4726.33518485073\n",
            "Episode 56 Kills - Red: 5, Blue: 25\n",
            "Episode 57, Total Reward: -4399.170175249688\n",
            "Episode 57 Kills - Red: 7, Blue: 38\n",
            "Episode 58, Total Reward: -4356.000169229694\n",
            "Episode 58 Kills - Red: 2, Blue: 28\n",
            "Episode 59, Total Reward: -4372.585167727433\n",
            "Episode 59 Kills - Red: 2, Blue: 20\n",
            "Episode 60, Total Reward: -4238.2501649763435\n",
            "Episode 60 Kills - Red: 5, Blue: 30\n",
            "Episode 61, Total Reward: -4444.945173190907\n",
            "Episode 61 Kills - Red: 3, Blue: 29\n",
            "Episode 62, Total Reward: -4607.170180000365\n",
            "Episode 62 Kills - Red: 4, Blue: 27\n",
            "Episode 63, Total Reward: -4481.190176308155\n",
            "Episode 63 Kills - Red: 3, Blue: 34\n"
          ]
        }
      ],
      "source": [
        "env = battle_v4.env(map_size=45, render_mode=\"rgb-array\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "observation_shape = env.observation_space(\"red_0\").shape\n",
        "action_shape = env.action_space(\"red_0\").n\n",
        "env.reset()\n",
        "\n",
        "# Initialize the VDN wrapper for centralized training\n",
        "blue_agents = [agent for agent in env.agents if agent.startswith(\"blue\")]\n",
        "vdn = VDN(observation_shape, action_shape, blue_agents, device=device)\n",
        "red_agent = RandomAgent(env.action_space(\"red_0\"))\n",
        "\n",
        "# Train the VDN algorithm\n",
        "train_vdn(env, red_agent, vdn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNg476OMMkxc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "rl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
